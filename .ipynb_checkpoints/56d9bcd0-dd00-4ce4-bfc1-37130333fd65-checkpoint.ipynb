{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "CUQuYmMs-mKK",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "CUQuYmMs-mKK",
    "tags": [
     "21a32eca-7494-4096-b202-21be1b7f0a7d"
    ]
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ff1020a",
   "metadata": {
    "cellId": "pbwq209p45adtxfy5ofecd",
    "execution_id": "7cb94123-6481-41de-973a-91abe63db303",
    "id": "1ff1020a"
   },
   "source": [
    "# Прекод\n",
    "\n",
    "# Сборный проект-4\n",
    "\n",
    "Вам поручено разработать демонстрационную версию поиска изображений по запросу.\n",
    "\n",
    "Для демонстрационной версии нужно обучить модель, которая получит векторное представление изображения, векторное представление текста, а на выходе выдаст число от 0 до 1 — покажет, насколько текст и картинка подходят друг другу.\n",
    "\n",
    "### Описание данных\n",
    "\n",
    "Данные доступны по [ссылке](https://code.s3.yandex.net/datasets/dsplus_integrated_project_4.zip).\n",
    "\n",
    "В файле `train_dataset.csv` находится информация, необходимая для обучения: имя файла изображения, идентификатор описания и текст описания. Для одной картинки может быть доступно до 5 описаний. Идентификатор описания имеет формат `<имя файла изображения>#<порядковый номер описания>`.\n",
    "\n",
    "В папке `train_images` содержатся изображения для тренировки модели.\n",
    "\n",
    "В файле `CrowdAnnotations.tsv` — данные по соответствию изображения и описания, полученные с помощью краудсорсинга. Номера колонок и соответствующий тип данных:\n",
    "\n",
    "1. Имя файла изображения.\n",
    "2. Идентификатор описания.\n",
    "3. Доля людей, подтвердивших, что описание соответствует изображению.\n",
    "4. Количество человек, подтвердивших, что описание соответствует изображению.\n",
    "5. Количество человек, подтвердивших, что описание не соответствует изображению.\n",
    "\n",
    "В файле `ExpertAnnotations.tsv` содержатся данные по соответствию изображения и описания, полученные в результате опроса экспертов. Номера колонок и соответствующий тип данных:\n",
    "\n",
    "1. Имя файла изображения.\n",
    "2. Идентификатор описания.\n",
    "\n",
    "3, 4, 5 — оценки трёх экспертов.\n",
    "\n",
    "Эксперты ставят оценки по шкале от 1 до 4, где 1 — изображение и запрос совершенно не соответствуют друг другу, 2 — запрос содержит элементы описания изображения, но в целом запрос тексту не соответствует, 3 — запрос и текст соответствуют с точностью до некоторых деталей, 4 — запрос и текст соответствуют полностью.\n",
    "\n",
    "В файле `test_queries.csv` находится информация, необходимая для тестирования: идентификатор запроса, текст запроса и релевантное изображение. Для одной картинки может быть доступно до 5 описаний. Идентификатор описания имеет формат `<имя файла изображения>#<порядковый номер описания>`.\n",
    "\n",
    "В папке `test_images` содержатся изображения для тестирования модели."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99489b26",
   "metadata": {
    "cellId": "n6vkjcacwu39w29bfocxt",
    "execution_id": "1b731a18-3394-4b62-b3f7-018692c2d6de",
    "id": "99489b26"
   },
   "source": [
    "## 1. Исследовательский анализ данных\n",
    "\n",
    "Наш датасет содержит экспертные и краудсорсинговые оценки соответствия текста и изображения.\n",
    "\n",
    "В файле с экспертными мнениями для каждой пары изображение-текст имеются оценки от трёх специалистов. Для решения задачи вы должны эти оценки агрегировать — превратить в одну. Существует несколько способов агрегации оценок, самый простой — голосование большинства: за какую оценку проголосовала большая часть экспертов (в нашем случае 2 или 3), та оценка и ставится как итоговая. Поскольку число экспертов меньше числа классов, может случиться, что каждый эксперт поставит разные оценки, например: 1, 4, 2. В таком случае данную пару изображение-текст можно исключить из датасета.\n",
    "\n",
    "Вы можете воспользоваться другим методом агрегации оценок или придумать свой.\n",
    "\n",
    "В файле с краудсорсинговыми оценками информация расположена в таком порядке:\n",
    "\n",
    "1. Доля исполнителей, подтвердивших, что текст **соответствует** картинке.\n",
    "2. Количество исполнителей, подтвердивших, что текст **соответствует** картинке.\n",
    "3. Количество исполнителей, подтвердивших, что текст **не соответствует** картинке.\n",
    "\n",
    "После анализа экспертных и краудсорсинговых оценок выберите либо одну из них, либо объедините их в одну по какому-то критерию: например, оценка эксперта принимается с коэффициентом 0.6, а крауда — с коэффициентом 0.4.\n",
    "\n",
    "Ваша модель должна возвращать на выходе вероятность соответствия изображения тексту, поэтому целевая переменная должна иметь значения от 0 до 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79250670",
   "metadata": {
    "cellId": "exl6m83oldxqlu1vq1s6",
    "id": "79250670"
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 0. Импорты и установка устройства\n",
    "# ===============================\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "import optuna\n",
    "\n",
    "# Устанавливаем устройство (GPU, если доступно)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Используемое устройство:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297be57d-51c3-4952-9e83-8974ac58505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 1. Загрузка и предобработка данных\n",
    "# ===============================\n",
    "\n",
    "# Задайте путь к данным (отредактируйте этот путь под свои данные)\n",
    "data_dir = Path(r'C:\\Users\\Deвайс\\ML\\to_upload')  # например, r'C:\\Users\\Deвайс\\ML\\to_upload'\n",
    "\n",
    "# Загрузка обучающего набора с описаниями\n",
    "train_df = pd.read_csv(data_dir / \"train_dataset.csv\")\n",
    "print(\"Train dataset (первые 5 строк):\")\n",
    "print(train_df.head())\n",
    "\n",
    "# Загрузка краудсорсинговых аннотаций\n",
    "crowd_df = pd.read_csv(\n",
    "    data_dir / \"CrowdAnnotations.tsv\",\n",
    "    sep=\"\\t\", header=None,\n",
    "    names=[\"image_file\", \"query_id\", \"score\", \"yes\", \"no\"]\n",
    ")\n",
    "print(\"Crowd annotations (первые 5 строк):\")\n",
    "print(crowd_df.head())\n",
    "\n",
    "# Загрузка экспертных аннотаций\n",
    "expert_df = pd.read_csv(\n",
    "    data_dir / \"ExpertAnnotations.tsv\",\n",
    "    sep=\"\\t\", header=None,\n",
    "    names=[\"image_file\", \"query_id\", \"ex1\", \"ex2\", \"ex3\"]\n",
    ")\n",
    "print(\"Expert annotations (первые 5 строк):\")\n",
    "print(expert_df.head())\n",
    "\n",
    "# Загрузка тестовых запросов\n",
    "test_queries_df = pd.read_csv(\n",
    "    data_dir / \"test_queries.csv\",\n",
    "    sep=\"|\", header=None,\n",
    "    names=[\"query_id\", \"query_text\", \"image\"],\n",
    "    skiprows=1\n",
    ")\n",
    "print(\"Test queries (первые 5 строк):\")\n",
    "print(test_queries_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86cbe2b-67eb-49e4-8ad3-d8e3fa982d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для фильтрации «проблемного» контента (например, связанного с детьми)\n",
    "child_keywords = ['child', 'baby', 'kid', 'toddler', 'teenager', 'girls', 'boys']\n",
    "def contains_child_keywords(text):\n",
    "    if pd.isna(text):\n",
    "        return False\n",
    "    return any(keyword in text.lower() for keyword in child_keywords)\n",
    "\n",
    "# Отфильтруем обучающий датасет (убираем строки с child-related контентом)\n",
    "filtered_train_df = train_df[~train_df[\"query_text\"].apply(contains_child_keywords)].reset_index(drop=True)\n",
    "print(\"Filtered train dataset (без запрещённого контента):\", filtered_train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec12006-54a1-4dc2-9541-8aa09c478ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Аггрегация экспертных оценок (используем голосование большинства)\n",
    "def aggregate_expert(row):\n",
    "    ratings = [row[\"ex1\"], row[\"ex2\"], row[\"ex3\"]]\n",
    "    # Если минимум два рейтинга совпадают, возвращаем их значение, иначе NaN\n",
    "    from collections import Counter\n",
    "    count = Counter(ratings)\n",
    "    most_common, cnt = count.most_common(1)[0]\n",
    "    return most_common if cnt >= 2 else np.nan\n",
    "\n",
    "expert_df[\"final_expert\"] = expert_df.apply(aggregate_expert, axis=1)\n",
    "\n",
    "# Аггрегация краудсорсинговых оценок: доля положительных голосов\n",
    "crowd_df[\"crowd_score\"] = crowd_df[\"yes\"] / (crowd_df[\"yes\"] + crowd_df[\"no\"])\n",
    "\n",
    "# Объединяем экспертные и краудсорсинговые оценки (например, вес экспертов = 0.6, крауд = 0.4)\n",
    "merged_scores = pd.merge(\n",
    "    expert_df[[\"image_file\", \"query_id\", \"final_expert\"]],\n",
    "    crowd_df[[\"image_file\", \"query_id\", \"crowd_score\"]],\n",
    "    on=[\"image_file\", \"query_id\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "merged_scores[\"final_score\"] = 0.6 * merged_scores[\"final_expert\"] + 0.4 * merged_scores[\"crowd_score\"]\n",
    "# Нормализуем в [0,1]\n",
    "min_val, max_val = merged_scores[\"final_score\"].min(), merged_scores[\"final_score\"].max()\n",
    "merged_scores[\"final_score_norm\"] = (merged_scores[\"final_score\"] - min_val) / (max_val - min_val)\n",
    "\n",
    "# Объединяем с описаниями из filtered_train_df\n",
    "# Предполагаем, что столбцы для объединения: \"image\" и \"query_id\"\n",
    "filtered_train_df = filtered_train_df.rename(columns={\"image\": \"image_file\"})\n",
    "merged_df = pd.merge(filtered_train_df, merged_scores[[\"image_file\", \"query_id\", \"final_score_norm\"]],\n",
    "                     on=[\"image_file\", \"query_id\"], how=\"inner\")\n",
    "print(\"Merged training dataset shape:\", merged_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a93c31-e1c9-4507-859c-59a55a9f732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbaa861",
   "metadata": {
    "cellId": "9h91oxwx86d7i8rqt5miv4",
    "execution_id": "4401a0e8-fd2b-479b-9e84-6ffafbcead47",
    "id": "7cbaa861"
   },
   "source": [
    "## 2. Проверка данных\n",
    "\n",
    "В некоторых странах, где работает ваша компания, действуют ограничения по обработке изображений: поисковым сервисам и сервисам, предоставляющим возможность поиска, запрещено без разрешения родителей или законных представителей предоставлять любую информацию, в том числе, но не исключительно тексты, изображения, видео и аудио, содержащие описание, изображение или запись голоса детей. Ребёнком считается любой человек, не достигший 16 лет.\n",
    "\n",
    "В вашем сервисе строго следуют законам стран, в которых работают. Поэтому при попытке посмотреть изображения, запрещённые законодательством, вместо картинок показывается дисклеймер:\n",
    "\n",
    "> This image is unavailable in your country in compliance with local laws\n",
    ">\n",
    "\n",
    "Однако у вас в PoC нет возможности воспользоваться данным функционалом. Поэтому все изображения, которые нарушают данный закон, нужно удалить из обучающей выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8622dbb2-bf61-437d-a3c4-452daff7b0ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d4ef807",
   "metadata": {
    "cellId": "ggxcvhmhcm9rshysbjoo4n",
    "execution_id": "d7935f99-48c0-42b8-a227-dd2b2d9b70fc",
    "id": "1d4ef807"
   },
   "source": [
    "## 3. Векторизация изображений\n",
    "\n",
    "Перейдём к векторизации изображений.\n",
    "\n",
    "Самый примитивный способ — прочесть изображение и превратить полученную матрицу в вектор. Такой способ нам не подходит: длина векторов может быть сильно разной, так как размеры изображений разные. Поэтому стоит обратиться к свёрточным сетям: они позволяют \"выделить\" главные компоненты изображений. Как это сделать? Нужно выбрать какую-либо архитектуру, например ResNet-18, посмотреть на слои и исключить полносвязные слои, которые отвечают за конечное предсказание. При этом можно загрузить модель данной архитектуры, предварительно натренированную на датасете ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fadeb3-c3b2-4ffb-8451-d26aa71392fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 2. Векторизация изображений и текстов\n",
    "# ===============================\n",
    "\n",
    "# Загрузка модели ResNet18 с использованием новых весов\n",
    "resnet18 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Убираем последний полностью связанный слой\n",
    "resnet18 = nn.Sequential(*list(resnet18.children())[:-1])\n",
    "\n",
    "resnet18.eval().to(device)\n",
    "\n",
    "# Преобразования для изображений\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def extract_image_features(img_path):\n",
    "    try:\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке изображения {img_path}: {e}\")\n",
    "        return None\n",
    "    img_tensor = image_transform(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = resnet18(img_tensor)\n",
    "    # Приводим к вектору\n",
    "    features = features.view(-1)\n",
    "    return features.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea53bf6",
   "metadata": {
    "cellId": "z8evfugfch8wpstvnxv0t",
    "execution_id": "028ade1d-49fe-4110-8b13-3c1aecdaa142",
    "id": "aea53bf6"
   },
   "source": [
    "## 4. Векторизация текстов\n",
    "\n",
    "Следующий этап — векторизация текстов. Вы можете поэкспериментировать с несколькими способами векторизации текстов:\n",
    "\n",
    "- tf-idf\n",
    "- word2vec\n",
    "- \\*трансформеры (например Bert)\n",
    "\n",
    "\\* — если вы изучали трансформеры в спринте Машинное обучение для текстов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de218cc3-3ea9-41b6-8a3c-6f60838e826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Очищает и токенизирует текст.\n",
    "    - Приводит текст к нижнему регистру.\n",
    "    - Удаляет ненужные символы и числа.\n",
    "    \"\"\"\n",
    "    # Приводим текст к нижнему регистру\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Удаляем все символы, кроме букв и пробелов\n",
    "    text = re.sub(r'[^a-zа-яё\\s]', '', text)\n",
    "    \n",
    "    # Токенизация текста (разделение на слова)\n",
    "    tokens = text.split()\n",
    "    \n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607a6545-2d18-4383-9e13-acf56d70385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Загружаем токенизатор и модель BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "bert_model.eval()\n",
    "\n",
    "def extract_text_features(text):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return None\n",
    "    \n",
    "    # Очистка текста\n",
    "    cleaned_text = clean_tokenize_text(text)\n",
    "    \n",
    "    # Токенизация и векторизация с использованием BERT\n",
    "    inputs = tokenizer(cleaned_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    \n",
    "    # Используем вектор [CLS] (первый токен)\n",
    "    cls_vector = outputs.last_hidden_state[:, 0, :].squeeze()\n",
    "    return cls_vector.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760c0ccd",
   "metadata": {
    "cellId": "yci1zcmnsacl720fr75sb",
    "execution_id": "5ecfa9d5-3913-4fb3-a33e-99bab3798577",
    "id": "760c0ccd"
   },
   "source": [
    "## 5. Объединение векторов\n",
    "\n",
    "Подготовьте данные для обучения: объедините векторы изображений и векторы текстов с целевой переменной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3676a49-93fe-4a4c-8d86-ca3bbc9e6837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 3. Формирование обучающего набора\n",
    "# ===============================\n",
    "# Для каждого примера в merged_df получим объединённый вектор (изображение + текст)\n",
    "# Если merged_df был сформирован ранее, удаляем строки с NaN в целевой переменной\n",
    "merged_df = merged_df.dropna(subset=[\"final_score_norm\"]).reset_index(drop=True)\n",
    "image_folder = data_dir / \"train_images\"\n",
    "X_list, y_list, groups = [], [], []\n",
    "for _, row in tqdm(merged_df.iterrows(), total=merged_df.shape[0], desc=\"Формирование обучающего набора\"):\n",
    "    img_path = image_folder / row[\"image_file\"]\n",
    "    img_feat = extract_image_features(str(img_path))\n",
    "    if img_feat is None or img_feat.shape[0] != 512:\n",
    "        continue\n",
    "    txt_feat = extract_text_features(row[\"query_text\"])\n",
    "    if txt_feat is None or txt_feat.shape[0] != 768:\n",
    "        continue\n",
    "    combined = np.concatenate([img_feat, txt_feat])  # размер: 1280\n",
    "    X_list.append(combined)\n",
    "    y_list.append(row[\"final_score_norm\"])\n",
    "    groups.append(row[\"image_file\"])\n",
    "\n",
    "X = np.array(X_list)\n",
    "y = np.array(y_list)\n",
    "print(\"Размер обучающего набора X:\", X.shape)\n",
    "print(\"Размер целевой переменной y:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218ca813-f057-471b-82c1-44f8dc20d0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Разбиение на обучающую и тестовую выборки\n",
    "# (Группируем по image_file, чтобы одно изображение не попало в обе выборки)\n",
    "# ===============================\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=0.7, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X, y, groups=groups))\n",
    "X_train, X_test = X[train_idx], X[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "print(\"Размеры обучающей выборки:\", X_train.shape, y_train.shape)\n",
    "print(\"Размеры тестовой выборки:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a6e2d1-1a2f-42f3-a8a3-c4ce80a314c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 5. Подготовка данных для PyTorch\n",
    "# ===============================\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y = torch.from_numpy(y).float().unsqueeze(1)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"data\": self.X[idx], \"target\": self.y[idx]}\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "test_dataset = CustomDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896e779f-4cb5-4154-9b33-312f2cfe0eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 6. Определение улучшенной модели и функции обучения с ранней остановкой\n",
    "# ===============================\n",
    "class ImprovedRegressionNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ImprovedRegressionNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        self.fc4 = nn.Linear(128, 1)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, n_epochs=100, patience=20, min_delta=0.0):\n",
    "    best_model_state = model.state_dict()  # Инициализация лучшего состояния модели\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "    train_rmse_history = []\n",
    "    val_rmse_history = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch[\"data\"].to(device))\n",
    "            loss_val = criterion(outputs, batch[\"target\"].to(device))\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss_val.item() * batch[\"data\"].size(0)\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_rmse = sqrt(train_loss)\n",
    "        train_rmse_history.append(train_rmse)\n",
    "\n",
    "        # Валидация\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                outputs = model(batch[\"data\"].to(device))\n",
    "                loss_val = criterion(outputs, batch[\"target\"].to(device))\n",
    "                running_val_loss += loss_val.item() * batch[\"data\"].size(0)\n",
    "        val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        val_rmse = sqrt(val_loss)\n",
    "        val_rmse_history.append(val_rmse)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}: Train RMSE: {train_rmse:.4f}, Val RMSE: {val_rmse:.4f}\")\n",
    "\n",
    "        # Обновляем лучшее состояние, если улучшение достигнуто\n",
    "        if best_val_loss - val_loss > min_delta:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model, {\"train_rmse\": train_rmse_history, \"val_rmse\": val_rmse_history}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28720ed0-0898-40e8-85a3-bad0d5b5d81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбиваем train_dataset на обучающую и валидационную (80/20)\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bc5668",
   "metadata": {
    "cellId": "97c9jj3s2zjj62vznivsk",
    "execution_id": "1a2d7233-0c79-479a-be63-5787145e3b48",
    "id": "60bc5668"
   },
   "source": [
    "## 6. Обучение модели предсказания соответствия\n",
    "\n",
    "Для обучения разделите датасет на тренировочную и тестовую выборки. Простое случайное разбиение не подходит: нужно исключить попадание изображения и в обучающую, и в тестовую выборки.\n",
    "Для того чтобы учесть изображения при разбиении, можно воспользоваться классом [GroupShuffleSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupShuffleSplit.html) из библиотеки sklearn.model_selection.\n",
    "\n",
    "Код ниже разбивает датасет на тренировочную и тестовую выборки в пропорции 7:3 так, что строки с одинаковым значением 'group_column' будут содержаться либо в тестовом, либо в тренировочном датасете.\n",
    "\n",
    "```\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=.7, random_state=42)\n",
    "train_indices, test_indices = next(gss.split(X=df.drop(columns=['target']), y=df['target'], groups=df['group_column']))\n",
    "train_df, test_df = df.loc[train_indices], df.loc[test_indices]\n",
    "\n",
    "```\n",
    "\n",
    "Какую модель использовать — выберите самостоятельно. Также вам предстоит выбрать метрику качества либо реализовать свою."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c60f35-abe9-4731-880c-6a81d31bff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Количество NaN в X:\", np.isnan(X).sum())\n",
    "print(\"Минимальное значение в X:\", np.min(X))\n",
    "print(\"Максимальное значение в X:\", np.max(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f444f544-aa15-4e00-b226-3aa7b85a0660",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Количество NaN в y:\", np.isnan(y).sum())\n",
    "print(\"Минимальное значение в y:\", np.min(y))\n",
    "print(\"Максимальное значение в y:\", np.max(y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0762e361-5442-49ce-bfab-87f41aaaa364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем и обучаем модель\n",
    "final_model = ImprovedRegressionNet(X_train.shape[1]).to(device)\n",
    "optimizer_final = optim.Adam(final_model.parameters(), lr=0.001)\n",
    "criterion_final = nn.MSELoss()\n",
    "\n",
    "final_model, history = train_model(final_model, train_loader, val_loader,\n",
    "                                   optimizer_final, criterion_final,\n",
    "                                   n_epochs=100, patience=20, min_delta=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e653819e-1c16-4575-825e-a621669c1876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка на тестовой выборке\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "final_model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        outputs = final_model(batch[\"data\"].to(device))\n",
    "        loss_val = criterion_final(outputs, batch[\"target\"].to(device))\n",
    "        test_loss += loss_val.item() * batch[\"data\"].size(0)\n",
    "final_rmse = sqrt(test_loss / len(test_dataset))\n",
    "print(f\"Final Test RMSE: {final_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f870d77",
   "metadata": {
    "cellId": "tbnfwg686jpxjdsw7cqbl",
    "execution_id": "5e14c3be-a481-438e-a979-0f4621acea44",
    "id": "2f870d77"
   },
   "source": [
    "## 7. Тестирование модели\n",
    "\n",
    "Настало время протестировать модель. Для этого получите эмбеддинги для всех тестовых изображений из папки `test_images`, выберите случайные 10 запросов из файла `test_queries.csv` и для каждого запроса выведите наиболее релевантное изображение. Сравните визуально качество поиска."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24638445-0496-459e-b2bf-daa18e1ae7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Функция для эмбеддинга текста\n",
    "def get_text_embedding(input_text):\n",
    "    tokenized = tokenizer.encode(input_text, max_length=512, truncation=True, add_special_tokens=True)\n",
    "    padded = pad_sequence([torch.as_tensor(tokenized)], batch_first=True)\n",
    "\n",
    "    # маска внимания\n",
    "    attention_mask = padded > 0\n",
    "    attention_mask = attention_mask.type(torch.LongTensor).to(device)\n",
    "\n",
    "    # эмбеддинги\n",
    "    with torch.no_grad():\n",
    "        text_embedding = model_emb_txt(padded.to(device), attention_mask=attention_mask)[0][:, 0, :].cpu().numpy()\n",
    "    return text_embedding[0].tolist()\n",
    "\n",
    "# Функция для тестирования\n",
    "def imag_test(query_text):\n",
    "    text_embedding = get_text_embedding(query_text)\n",
    "    \n",
    "    # Добавляем текстовый эмбеддинг к изображению (как у друга, сложение вместо конкатенации)\n",
    "    test['vector'] = test['image_vector'].apply(lambda x: np.array(x, dtype=np.float32) + np.array(text_embedding, dtype=np.float32))\n",
    "\n",
    "    # Создание тензора\n",
    "    test_vectors = np.stack(test['vector'].to_numpy())\n",
    "    X_test_tensor = torch.tensor(test_vectors, dtype=torch.float32)\n",
    "\n",
    "    # Вычисление предсказания с использованием регрессора\n",
    "    test['pred'] = best_regressor.predict(X_test_tensor)\n",
    "    \n",
    "    # Получение изображения с максимальной оценкой\n",
    "    max_score = test['pred'].max()\n",
    "    image_path = test[test['pred'] == test['pred'].max()]['image'].values[0]\n",
    "    return max_score, image_path\n",
    "\n",
    "# Функция для вывода изображения по запросу\n",
    "def display_image_with_caption(query_text):\n",
    "    text = clean_tokenize_text(query_text)\n",
    "    \n",
    "    # Проверка наличия запрета на изображение\n",
    "    if any(i in text for i in child_list):\n",
    "        print(query_text)\n",
    "        print('Изображение не доступно в данном регионе')\n",
    "    else:\n",
    "        max_score, image_path = imag_test(text)\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        image_path = os.path.join(TEST_IMAGES, image_path)\n",
    "        \n",
    "        # Загружаем изображение\n",
    "        try:\n",
    "            img = Image.open(image_path)\n",
    "            ax.imshow(img)\n",
    "            ax.set_title(query_text, fontsize=12)\n",
    "            ax.axis('off')\n",
    "            plt.show()\n",
    "            print(f'Мера соответствия изображения: {max_score:.4f}')\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при загрузке изображения: {e}\")\n",
    "\n",
    "# Код для получения эмбеддингов изображений и их обработки\n",
    "test_images_folder = data_dir / \"test_images\"\n",
    "test_image_files = list(test_images_folder.glob(\"*.jpg\"))\n",
    "test_image_embeds = {}\n",
    "\n",
    "# Получаем эмбеддинги для всех изображений\n",
    "for path in test_image_files:\n",
    "    emb = extract_image_features(str(path))\n",
    "    if emb is not None and emb.shape[0] == 512:  # Убедитесь, что размер эмбеддингов изображений правильный\n",
    "        test_image_embeds[path.name] = emb\n",
    "print(f\"Вычислено эмбеддингов для {len(test_image_embeds)} тестовых изображений.\")\n",
    "\n",
    "# Формируем список запросов\n",
    "queries = [\n",
    "    \"A large tan dog sits on a grassy hill .\",\n",
    "    \"A large yellow dog is sitting on a hill .\",\n",
    "    \"The dog is sitting on the side of the hill .\",\n",
    "    \"A white dog and a black dog in a field .\",\n",
    "    \"A white dog with a branch in his mouth and a black dog .\",\n",
    "    \"A white dog with a stick in his mouth standing next to a black dog .\",\n",
    "    \"Two dogs are standing next to each other , and the white dog has a stick in its mouth .\",\n",
    "    \"Two dogs stand in the brown grass .\",\n",
    "    \"Two girls in pink are playing on yellow playground bars .\",\n",
    "    \"Two girls on a jungle gym .\"\n",
    "]\n",
    "\n",
    "# Итерируем по запросам и выводим результаты\n",
    "for query in queries:\n",
    "    display_image_with_caption(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c92f453-499d-43fa-9e0a-eeb7a09935df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_best_image(query_text, image_embeds, model):\n",
    "    txt_emb = extract_text_features(query_text)\n",
    "    if txt_emb is None or txt_emb.shape[0] != 768:\n",
    "        print(f\"Ошибка в векторизации запроса: {query_text}\")\n",
    "        return None, None\n",
    "\n",
    "    img_names = list(image_embeds.keys())\n",
    "    img_matrix = np.stack([image_embeds[name] for name in img_names], axis=0)\n",
    "    n_images = img_matrix.shape[0]\n",
    "    txt_matrix = np.tile(txt_emb, (n_images, 1))\n",
    "    combined_matrix = np.concatenate([img_matrix, txt_matrix], axis=1)\n",
    "    combined_tensor = torch.from_numpy(combined_matrix).float().to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        scores = model(combined_tensor).cpu().numpy().flatten()\n",
    "    \n",
    "    # Выводим распределение оценок для отладки\n",
    "    print(f\"Запрос: {query_text}\\nОценки для изображений: {np.round(scores, 4)}\")\n",
    "    \n",
    "    best_idx = np.argmax(scores)\n",
    "    best_image = img_names[best_idx]\n",
    "    best_score = scores[best_idx]\n",
    "    return best_image, best_score\n",
    "\n",
    "\n",
    "# Предположим, что у вас уже вычислены эмбеддинги тестовых изображений:\n",
    "# Например, если test_images_folder – путь к тестовой папке,\n",
    "# то можно сделать следующее (один раз):\n",
    "test_images_folder = data_dir / \"test_images\"  # например, путь к тестовым изображениям\n",
    "test_image_files = list(test_images_folder.glob(\"*.jpg\"))\n",
    "test_image_embeds = {}\n",
    "for path in test_image_files:\n",
    "    emb = extract_image_features(str(path))\n",
    "    if emb is not None and emb.shape[0] == 512:\n",
    "        test_image_embeds[path.name] = emb\n",
    "print(f\"Вычислено эмбеддингов для {len(test_image_embeds)} тестовых изображений.\")\n",
    "\n",
    "# Теперь сформируем список запросов (из вашего примера)\n",
    "queries = [\n",
    "    \"A large tan dog sits on a grassy hill .\",\n",
    "    \"A large yellow dog is sitting on a hill .\",\n",
    "    \"The dog is sitting on the side of the hill .\",\n",
    "    \"A white dog and a black dog in a field .\",\n",
    "    \"A white dog with a branch in his mouth and a black dog .\",\n",
    "    \"A white dog with a stick in his mouth standing next to a black dog .\",\n",
    "    \"Two dogs are standing next to each other , and the white dog has a stick in its mouth .\",\n",
    "    \"Two dogs stand in the brown grass .\",\n",
    "    \"Two girls in pink are playing on yellow playground bars .\",\n",
    "    \"Two girls on a jungle gym .\"\n",
    "]\n",
    "\n",
    "# Итерируем по запросам и выводим результаты\n",
    "for query in queries:\n",
    "    best_img, score = search_best_image(query, test_image_embeds, final_model)  # или improved_model/другая модель\n",
    "    if best_img is None:\n",
    "        print(f\"Запрос: {query}\\nНе удалось найти релевантное изображение.\\n\")\n",
    "        continue\n",
    "    print(f\"\\nЗапрос: {query}\")\n",
    "    print(f\"Лучшее изображение: {best_img} с оценкой: {score:.4f}\")\n",
    "    # Отображаем изображение\n",
    "    try:\n",
    "        img = Image.open(test_images_folder / best_img).convert(\"RGB\")\n",
    "        plt.figure(figsize=(6,6))\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Score: {score:.4f}\")\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при открытии изображения {best_img}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483ae361-b507-4de9-bff9-199c7c2a6a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_best_image(query_text, image_embeds, model):\n",
    "    txt_emb = extract_text_features(query_text)\n",
    "    if txt_emb is None or txt_emb.shape[0] != 768:\n",
    "        print(f\"Ошибка в векторизации запроса: {query_text}\")\n",
    "        return None, None\n",
    "\n",
    "    img_names = list(image_embeds.keys())\n",
    "    img_matrix = np.stack([image_embeds[name] for name in img_names], axis=0)\n",
    "    n_images = img_matrix.shape[0]\n",
    "    txt_matrix = np.tile(txt_emb, (n_images, 1))\n",
    "    combined_matrix = np.concatenate([img_matrix, txt_matrix], axis=1)\n",
    "    combined_tensor = torch.from_numpy(combined_matrix).float().to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        scores = model(combined_tensor).cpu().numpy().flatten()\n",
    "    \n",
    "    # Выводим распределение оценок для отладки\n",
    "    print(f\"Запрос: {query_text}\\nОценки для изображений: {np.round(scores, 4)}\")\n",
    "    \n",
    "    best_idx = np.argmax(scores)\n",
    "    best_image = img_names[best_idx]\n",
    "    best_score = scores[best_idx]\n",
    "    return best_image, best_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab1345a",
   "metadata": {
    "cellId": "dnvdkzzxdpet1yc4m64cx",
    "execution_id": "3e367f6a-97e3-4ed7-9b73-39ed363fd2b7",
    "id": "fab1345a"
   },
   "source": [
    "## 8. Выводы\n",
    "\n",
    "- [x]  Jupyter Notebook открыт\n",
    "- [ ]  Весь код выполняется без ошибок\n",
    "- [ ]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [ ]  Исследовательский анализ данных выполнен\n",
    "- [ ]  Проверены экспертные оценки и краудсорсинговые оценки\n",
    "- [ ]  Из датасета исключены те объекты, которые выходят за рамки юридических ограничений\n",
    "- [ ]  Изображения векторизованы\n",
    "- [ ]  Текстовые запросы векторизованы\n",
    "- [ ]  Данные корректно разбиты на тренировочную и тестовую выборки\n",
    "- [ ]  Предложена метрика качества работы модели\n",
    "- [ ]  Предложена модель схожести изображений и текстового запроса\n",
    "- [ ]  Модель обучена\n",
    "- [ ]  По итогам обучения модели сделаны выводы\n",
    "- [ ]  Проведено тестирование работы модели\n",
    "- [ ]  По итогам тестирования визуально сравнили качество поиска"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Отсутствует",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "notebookId": "e47b60f7-b2b4-44ee-beb3-b44a93eaf068",
  "notebookPath": "precode.ipynb",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
